{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT on IMDB Sentiment Analysis\n",
    "\n",
    "这个文件在IMDB上进行情感分类，熟悉transformers库的使用，测试以下模型的表现：\n",
    "\n",
    "* BERT\n",
    "\n",
    "目前的进度：\n",
    "\n",
    "* 未完成\n",
    "\n",
    "问题：\n",
    "* BERT用来分类时后面的模型怎么接？是直接将[CLS]接入线性层，还是将所有token的表示输入一个RNN？\n",
    "\n",
    "参考：\n",
    "* [bentrevett/pytorch-sentiment-analysis/6 - Transformers for Sentiment Analysis.ipynb](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/6%20-%20Transformers%20for%20Sentiment%20Analysis.ipynb)\n",
    "* [torchtext使用--Transformer的IMDB情感分析](https://blog.csdn.net/weixin_43301333/article/details/105893946)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import datasets\n",
    "from torchtext import data\n",
    "import numpy as np\n",
    "import random\n",
    "from torch import nn,optim\n",
    "from sklearn import metrics\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda=torch.cuda.is_available()\n",
    "device=torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "SEED = 1234\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可供调整的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=128\n",
    "d_hidden=256\n",
    "d_output=2\n",
    "n_layers=2\n",
    "bidirectional=True\n",
    "dropout=0.25\n",
    "model_name='1'\n",
    "max_epochs=1\n",
    "require_improvement=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "**首先为了使用BERT，我们需要使用配套的tokennizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该分词器中含30522个token。\n",
    "\n",
    "我们可以简单看一下这个分词器的效果，可以调用分词器的`tokenizer.tokenize`方法，对输入的句子分词。\n",
    "\n",
    "可以调用`tokenizer.convert_tokens_to_ids`方法,将句子转换成序号."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'how', 'are', 'you', '?']\n",
      "[7592, 2088, 2129, 2024, 2017, 1029]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize('Hello WORLD how ARE yoU?')\n",
    "print(tokens)\n",
    "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来可以看看词典中的特殊字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [SEP] [PAD] [UNK]\n",
      "101 102 0 100\n"
     ]
    }
   ],
   "source": [
    "init_token = tokenizer.cls_token\n",
    "eos_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "print(init_token, eos_token, pad_token, unk_token)\n",
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer类型的模型均有最大文本长度，在词典`tokenizer.max_model_input_sizes`存储着多个模型对应的最大长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
    "print(max_input_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到最大长度是512。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**我们需要定义一个分词器，以传入field。**\n",
    "\n",
    "注意这里需要留两位给两个特殊token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**定义field**，需要注意的是我们传入了分词方法和预处理方法：`tokenize = tokenize_and_cut`, `preprocessing = tokenizer.convert_tokens_to_ids`。\n",
    "\n",
    "`use_vocab = False`是由于我们已经建立的词典，另外需要传入特殊字符的序号。注意这里传入的是序号而非字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(batch_first = True,\n",
    "                  use_vocab = False,\n",
    "                  tokenize = tokenize_and_cut,\n",
    "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                  init_token = init_token_idx,\n",
    "                  eos_token = eos_token_idx,\n",
    "                  pad_token = pad_token_idx,\n",
    "                  unk_token = unk_token_idx)\n",
    "\n",
    "LABEL = data.LabelField(dtype = torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**载入数据并进行划分。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 20000\n",
      "Number of validation examples: 5000\n",
      "Number of testing examples: 25000\n",
      "{'text': [2023, 3185, 2038, 2288, 2000, 2022, 2028, 1997, 1996, 5409, 1045, 2031, 2412, 2464, 2191, 2009, 2000, 4966, 999, 999, 999, 1996, 2466, 2240, 2453, 2031, 13886, 2065, 1996, 2143, 2018, 2062, 4804, 1998, 4898, 2008, 2052, 2031, 3013, 1996, 14652, 1998, 5305, 2135, 5019, 2008, 1045, 3811, 14046, 3008, 2006, 1012, 1012, 1012, 1012, 2021, 1996, 2466, 2240, 2003, 2066, 1037, 6065, 8854, 1012, 2065, 2045, 2001, 2107, 1037, 2518, 2004, 1037, 3298, 27046, 3185, 9338, 1011, 2023, 2028, 2052, 2031, 22057, 2013, 2008, 1012, 2009, 6966, 2033, 1037, 2843, 1997, 1996, 4248, 2666, 3152, 2008, 2020, 2404, 2041, 1999, 1996, 3624, 1005, 1055, 1010, 3532, 5896, 3015, 1998, 7467, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 2069, 21082, 3494, 1999, 1996, 2878, 3185, 2001, 1996, 15812, 1998, 13570, 1012, 1996, 2717, 1997, 1996, 2143, 1010, 2071, 2031, 4089, 2042, 2081, 2011, 2690, 2082, 2336, 1012, 1045, 2507, 2023, 2143, 1037, 5790, 1997, 1015, 2004, 2009, 2003, 5621, 9643, 1998, 2187, 2026, 2972, 2155, 2007, 1037, 3168, 1997, 2108, 22673, 1012, 2026, 6040, 1011, 2123, 1005, 1056, 3422, 2009, 999, 999, 999], 'label': 'neg'}\n",
      "['this', 'movie', 'has', 'got', 'to', 'be', 'one', 'of', 'the', 'worst', 'i', 'have', 'ever', 'seen', 'make', 'it', 'to', 'dvd', '!', '!', '!', 'the', 'story', 'line', 'might', 'have', 'clicked', 'if', 'the', 'film', 'had', 'more', 'funding', 'and', 'writers', 'that', 'would', 'have', 'cut', 'the', 'nonsense', 'and', 'sick', '##ly', 'scenes', 'that', 'i', 'highly', 'caution', 'parents', 'on', '.', '.', '.', '.', 'but', 'the', 'story', 'line', 'is', 'like', 'a', 'loose', 'cannon', '.', 'if', 'there', 'was', 'such', 'a', 'thing', 'as', 'a', 'drive', 'thru', 'movie', 'maker', '-', 'this', 'one', 'would', 'have', 'sprung', 'from', 'that', '.', 'it', 'reminded', 'me', 'a', 'lot', 'of', 'the', 'quick', '##ie', 'films', 'that', 'were', 'put', 'out', 'in', 'the', '1960', \"'\", 's', ',', 'poor', 'script', 'writing', 'and', 'filming', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'the', 'only', 'sensible', 'characters', 'in', 'the', 'whole', 'movie', 'was', 'the', 'bartender', 'and', 'beaver', '.', 'the', 'rest', 'of', 'the', 'film', ',', 'could', 'have', 'easily', 'been', 'made', 'by', 'middle', 'school', 'children', '.', 'i', 'give', 'this', 'film', 'a', 'rating', 'of', '1', 'as', 'it', 'is', 'truly', 'awful', 'and', 'left', 'my', 'entire', 'family', 'with', 'a', 'sense', 'of', 'being', 'cheated', '.', 'my', 'advice', '-', 'don', \"'\", 't', 'watch', 'it', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "train_data, valid_data = train_data.split(split_ratio=0.8)\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')\n",
    "#一个样本\n",
    "print(vars(train_data.examples[0]))\n",
    "tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[0])['text'])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尽管不需要建立词典，但是标签的序号需要确定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'neg': 0, 'pos': 1})\n"
     ]
    }
   ],
   "source": [
    "LABEL.build_vocab(train_data)\n",
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iterator需要建立。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = bs, \n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497d5dff7cbc431eabdf33c944584011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b46262b76dd4bb68d128eebdd1d5d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于BertModel来说，它的返回由四个部分组成：\n",
    "* last_hidden_state ：（batch,seq,hidden_size）\n",
    "整个输入的句子每一个token的隐层输出，也是我们这里将要用到的，可以将它作为embedding的替代\n",
    "* pooler_output：（batch,hidden_size）\n",
    "输入句子第一个token的最高隐层。也就是[CLS]标记提取到的最终的句对级别的抽象信息。对于BERT的预训练来说，这个隐层信息将作为Next Sentence prediction任务的输入。然而我们这里将不会用到它，因为它对于情感分析来说效果不是很好。\n",
    "This output is usually not a good summary of the semantic content of the input, you’re often better with averaging or pooling the sequence of hidden-states for the whole input sequence.\n",
    "* hidden_states：\n",
    "一个元组，里面每个元素都是(batch,seq,hidden_size) 大小的FloatTensor，分别代表每一层的隐层和初始embedding的和\n",
    "* attentions:\n",
    "一个元组，里面每个元素都是(batch,num_heads,seq,seq) 大小的FloatTensor，分别表示每一层的自注意力分数。\n",
    "\n",
    "在本实验中，我们仅用BERT作为一个特征抽取器，固定参数，不去进行微调。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTGRUSentiment(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers,\n",
    "                 bidirectional,\n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        #固定参数，不对BERT进行微调\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = False\n",
    "        \n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        \n",
    "        self.rnn = nn.GRU(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = bidirectional,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "        \n",
    "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [batch size, sent len]\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            #固定参数，不对BERT进行微调\n",
    "            embedded = self.bert(text)[0]\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        _, hidden = self.rnn(embedded)\n",
    "        \n",
    "        #hidden = [n layers * n directions, batch size, hidden_dim]\n",
    "        \n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "                \n",
    "        #hidden = [batch size, hid dim]\n",
    "        \n",
    "        output = self.out(hidden)\n",
    "        \n",
    "        #output = [batch size, out dim]\n",
    "        \n",
    "        return output\n",
    "    \n",
    "model = BERTGRUSentiment(bert,\n",
    "                         d_hidden,\n",
    "                         d_output,\n",
    "                         n_layers,\n",
    "                         bidirectional,\n",
    "                         dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn.weight_ih_l0\n",
      "rnn.weight_hh_l0\n",
      "rnn.bias_ih_l0\n",
      "rnn.bias_hh_l0\n",
      "rnn.weight_ih_l0_reverse\n",
      "rnn.weight_hh_l0_reverse\n",
      "rnn.bias_ih_l0_reverse\n",
      "rnn.bias_hh_l0_reverse\n",
      "rnn.weight_ih_l1\n",
      "rnn.weight_hh_l1\n",
      "rnn.bias_ih_l1\n",
      "rnn.bias_hh_l1\n",
      "rnn.weight_ih_l1_reverse\n",
      "rnn.weight_hh_l1_reverse\n",
      "rnn.bias_ih_l1_reverse\n",
      "rnn.bias_hh_l1_reverse\n",
      "out.weight\n",
      "out.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():                \n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试一下能否跑通"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if use_cuda:\n",
    "        model.to(device)\n",
    "        criterion.to(device)\n",
    "    for batch in train_iterator:\n",
    "        #print(batch.text.shape)\n",
    "        #print(batch.label)\n",
    "        logits=model(batch.text)\n",
    "        print(logits.shape)\n",
    "        criterion(logits,batch.label)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_iter, dev_iter, test_iter):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #criterion = nn.BCEWithLogitsLoss()\n",
    "    if use_cuda:\n",
    "        criterion.cuda()\n",
    "\n",
    "    # 学习率指数衰减，每次epoch：学习率 = gamma * 学习率\n",
    "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    dev_best_loss = float('inf')\n",
    "    last_improve = 0  # 记录上次验证集loss下降的batch数\n",
    "    #writer = SummaryWriter(log_dir=config.log_path + '/' + time.strftime('%m-%d.%H.%M', time.localtime())+'_'+which_data+'_'+which_model+'_'+which_task+'_'+exp_number)\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        start_time = time.time()\n",
    "        train_loss=0\n",
    "        train_correct=0\n",
    "        # scheduler.step() # 学习率衰减\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            optimizer.zero_grad()\n",
    "            x,l=batch.text\n",
    "            y=batch.label\n",
    "            outputs = model(x,l)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #训练集的准确率\n",
    "            preds = torch.max(outputs, 1)[1]\n",
    "            #preds = torch.round(torch.sigmoid(outputs))\n",
    "            train_correct+=(y==preds).sum()\n",
    "            train_loss+=loss.item()\n",
    "        train_loss/=len(train_iterator)   #train_loss\n",
    "        train_acc=train_correct/len(train_iterator.dataset)   #train_acc\n",
    "            \n",
    "        #验证集\n",
    "        dev_acc, dev_loss = evaluate(model, dev_iter)\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        if dev_loss < dev_best_loss:\n",
    "            dev_best_loss = dev_loss\n",
    "            improve = '*'\n",
    "            last_improve=epoch\n",
    "            torch.save(model.state_dict(),model_name+'.pth')\n",
    "        else:\n",
    "            improve = ''\n",
    "        msg = 'Epoch: {0:>3},  Epoch Time: {1}m {2}s,  Train Loss: {3:>5.2},  Train Acc: {4:>6.2%},  Val Loss: {5:>5.2},  Val Acc: {6:>6.2%} {7}'\n",
    "        print(msg.format(epoch+1,epoch_mins, epoch_secs,train_loss, train_acc, dev_loss, dev_acc, improve))\n",
    "        #writer.add_scalar(\"loss/train\", loss.item(), total_batch)\n",
    "        #writer.add_scalar(\"loss/dev\", dev_loss, total_batch)\n",
    "        #writer.add_scalar(\"acc/train\", train_acc, total_batch)\n",
    "        #writer.add_scalar(\"acc/dev\", dev_acc, total_batch)\n",
    "\n",
    "        if epoch - last_improve > require_improvement:\n",
    "            # 验证集loss超过1epoch没下降，结束训练\n",
    "            print(\"No optimization for a long time, auto-stopping...\")\n",
    "            break\n",
    "    #writer.close()\n",
    "    #训练跑完了，使用最佳模型测试\n",
    "    model.load_state_dict(torch.load(model_name+'.pth'))\n",
    "    test(model, test_iter)\n",
    "\n",
    "def evaluate(model, data_iter, test=False):\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    with torch.no_grad():\n",
    "        for batch in data_iter:\n",
    "            x,l=batch.text\n",
    "            labels=batch.label\n",
    "            outputs = model(x,l)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            #loss=criterion(outputs,labels)\n",
    "            loss_total += loss\n",
    "            labels = labels.data.cpu().numpy()\n",
    "            predic = torch.max(outputs, 1)[1].cpu().numpy()\n",
    "            #predic=torch.round(torch.sigmoid(outputs)).cpu().numpy()\n",
    "            labels_all = np.append(labels_all, labels)\n",
    "            predict_all = np.append(predict_all, predic)\n",
    "    model.train()\n",
    "    acc = metrics.accuracy_score(labels_all, predict_all)\n",
    "    \n",
    "    if test:\n",
    "        report = metrics.classification_report(labels_all, predict_all, labels=[0,1],target_names=['pos','neg'], digits=4,output_dict=True)\n",
    "        confusion = metrics.confusion_matrix(labels_all, predict_all)\n",
    "        return acc, loss_total / len(data_iter), report, confusion\n",
    "    \n",
    "    return acc, loss_total / len(data_iter)\n",
    "\n",
    "\n",
    "def test(model, test_iter):\n",
    "    test_acc, test_loss, test_report, test_confusion = evaluate(model, test_iter, test=True)\n",
    "    msg = 'Test Loss: {0:>5.2},  Test Acc: {1:>6.2%}'\n",
    "    print(msg.format(test_loss, test_acc))\n",
    "    print(\"Precision, Recall and F1-Score...\")\n",
    "    print(test_report)\n",
    "    print(\"Confusion Matrix...\")\n",
    "    print(test_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original\n",
    "train(model,train_iterator,valid_iterator,test_iterator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
