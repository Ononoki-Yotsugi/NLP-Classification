{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9K3I2xacXWqq"
   },
   "source": [
    "# LSTM on IMDB Sentiment Analysis\n",
    "\n",
    "这个文件在IMDB上进行情感分类，练习torchtext等的使用，熟悉训练流程，并测试以下模型的表现：\n",
    "\n",
    "* LSTM\n",
    "\n",
    "相对前一个文件做了以下改进\n",
    "\n",
    "* 使用预训练词向量\n",
    "* 进行了pack，pad操作\n",
    "* 考虑了num_layer,dropout,momentum的影响\n",
    "\n",
    "目前的进度：\n",
    "\n",
    "* 未完成\n",
    "\n",
    "问题：\n",
    "* BucketIterator，shuffle，packandpad这几个怎么处理？（目前采用的是只在batch间shuffle，长度相近的在同一个batch，batch内按降序排列）\n",
    "* packpad操作之后hidden和output对应位置不一致？\n",
    "* Adam不需要指定学习率？\n",
    "\n",
    "\n",
    "参考：\n",
    "* [torchtext使用--updated IMDB](https://blog.csdn.net/weixin_43301333/article/details/105745053)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atw_K4Opf9p-"
   },
   "source": [
    "## Requirement\n",
    "* torchtext==0.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJSCPavXXWqz"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 734,
     "status": "ok",
     "timestamp": 1614414802753,
     "user": {
      "displayName": "义云",
      "photoUrl": "",
      "userId": "00944396917142299573"
     },
     "user_tz": -480
    },
    "id": "dwiSrbYnXWq0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import datasets\n",
    "from torchtext import data\n",
    "import numpy as np\n",
    "import random\n",
    "from torch import nn,optim\n",
    "from sklearn import metrics\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda=torch.cuda.is_available()\n",
    "device=torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyFvA66KXWq0"
   },
   "source": [
    "## 供调整的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 942,
     "status": "ok",
     "timestamp": 1614415802997,
     "user": {
      "displayName": "义云",
      "photoUrl": "",
      "userId": "00944396917142299573"
     },
     "user_tz": -480
    },
    "id": "wS8cEXVvXWq1"
   },
   "outputs": [],
   "source": [
    "lr=1e-3\n",
    "bs=3\n",
    "d_embed=100\n",
    "d_hidden=256\n",
    "d_output=2\n",
    "dropout=0.0\n",
    "max_epochs=10\n",
    "require_improvement=1\n",
    "n_layers=1\n",
    "bidirectional=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cr7-Kb3zXWq1"
   },
   "source": [
    "## 数据载入和处理\n",
    "\n",
    "在载入和处理数据部分采用了torchtext库。\n",
    "\n",
    "由于在colab上无法运行spacy，我们采用简单的按空格分词，spacy后续在服务器上跑时加进去。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1305877,
     "status": "ok",
     "timestamp": 1614414156195,
     "user": {
      "displayName": "义云",
      "photoUrl": "",
      "userId": "00944396917142299573"
     },
     "user_tz": -480
    },
    "id": "1s72npWTXWq2",
    "outputId": "4b199e2f-9b79-40b8-af0c-da9f04530556"
   },
   "outputs": [],
   "source": [
    "tokenize = lambda x: x.split()\n",
    "TEXT=data.Field(tokenize=tokenize,batch_first=True,include_lengths=True)\n",
    "LABEL=data.LabelField(dtype=torch.long)\n",
    "train_data,test_data=datasets.IMDB.splits(TEXT,LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1AhFvm4XWq2"
   },
   "source": [
    "**下面展示样本数量和一个样本。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 900,
     "status": "ok",
     "timestamp": 1614414345153,
     "user": {
      "displayName": "义云",
      "photoUrl": "",
      "userId": "00944396917142299573"
     },
     "user_tz": -480
    },
    "id": "RcNJ6HMxXWq2",
    "outputId": "aec64b98-2b5b-4501-8002-b30136bc4bd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 25000\n",
      "Number of testing examples: 25000\n",
      "['Bromwell', 'High', 'is', 'a', 'cartoon', 'comedy.', 'It', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life,', 'such', 'as', '\"Teachers\".', 'My', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'Bromwell', \"High's\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '\"Teachers\".', 'The', 'scramble', 'to', 'survive', 'financially,', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', \"teachers'\", 'pomp,', 'the', 'pettiness', 'of', 'the', 'whole', 'situation,', 'all', 'remind', 'me', 'of', 'the', 'schools', 'I', 'knew', 'and', 'their', 'students.', 'When', 'I', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school,', 'I', 'immediately', 'recalled', '.........', 'at', '..........', 'High.', 'A', 'classic', 'line:', 'INSPECTOR:', \"I'm\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers.', 'STUDENT:', 'Welcome', 'to', 'Bromwell', 'High.', 'I', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'Bromwell', 'High', 'is', 'far', 'fetched.', 'What', 'a', 'pity', 'that', 'it', \"isn't!\"]\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')\n",
    "print(vars(train_data.examples[0])['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adpCmyEPXWq3"
   },
   "source": [
    "有25000个训练样本和25000个测试样本，尽管这个数量比不太符合要求，但是这个任务比较简单，我们就这么来。\n",
    "\n",
    "一个样本是一个字典的形式，'text'中含有分词完毕的单词列表，'label'中含其标签（pos或neg）。\n",
    "\n",
    "**下面我们需要把训练样本中再分一些出来作为验证集。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 880,
     "status": "ok",
     "timestamp": 1614414353129,
     "user": {
      "displayName": "义云",
      "photoUrl": "",
      "userId": "00944396917142299573"
     },
     "user_tz": -480
    },
    "id": "UAoPnEQdXWq4",
    "outputId": "b1f2d1a2-b9e9-4803-e4a4-523fe73afa1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 20000\n",
      "Number of validation examples: 5000\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "#确保每次分割相同\n",
    "SEED = 1234\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "train_data,valid_data=train_data.split(split_ratio=0.8)\n",
    "\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puG1JBfGXWq4"
   },
   "source": [
    "**下面我们需要建立词典**\n",
    "\n",
    "**这里我们使用Glove的100维词向量初始化**\n",
    "\n",
    "这里词典最大长度是否需要指定？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1838,
     "status": "ok",
     "timestamp": 1614414359015,
     "user": {
      "displayName": "义云",
      "photoUrl": "",
      "userId": "00944396917142299573"
     },
     "user_tz": -480
    },
    "id": "WEefVjpaXWq5",
    "outputId": "78cb80ec-dc4e-4ec9-8728-818e619fc3cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 244103\n",
      "Unique tokens in LABEL vocabulary: 2\n",
      "最频繁的20个单词：\n",
      "[('the', 230093), ('a', 123651), ('and', 122106), ('of', 114226), ('to', 106104), ('is', 82638), ('in', 68335), ('I', 52549), ('that', 51704), ('this', 45786), ('it', 43665), ('/><br', 40779), ('was', 37439), ('as', 33966), ('with', 33196), ('for', 32883), ('The', 27080), ('but', 27064), ('on', 24547), ('movie', 24429)]\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_data,vectors='glove.6B.100d',unk_init=torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "d_vocab=len(TEXT.vocab)\n",
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n",
    "\n",
    "print('最频繁的20个单词：')\n",
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qFlMGk2XWq5"
   },
   "source": [
    "测试和验证文本中可能出现训练集中没有的单词，另外在训练时为了满足批量输入需要将所有或一个批次的文本长度对齐，因此上述字典的建立中会自动加入特殊标记_&lt;unk&gt;_ 和*&lt;pad&gt;* ，用来表示未知字符和填充字符。\n",
    "\n",
    "\n",
    "**下面我们需要建立迭代器**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 814,
     "status": "ok",
     "timestamp": 1614414561073,
     "user": {
      "displayName": "义云",
      "photoUrl": "",
      "userId": "00944396917142299573"
     },
     "user_tz": -480
    },
    "id": "vl5xshlgXWq6",
    "outputId": "b779f766-6037-420a-ee2e-888a1bb5185d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 403])\n",
      "tensor([[   756,      2,    498,  ..., 143978,   2526,   5409],\n",
      "        [  2338,   5804,  10313,  ...,      1,      1,      1],\n",
      "        [    49,   8307,      9,  ...,      1,      1,      1]])\n"
     ]
    }
   ],
   "source": [
    "train_iterator, valid_iterator, test_iterator =data.BucketIterator.splits(\n",
    "    (train_data,valid_data,test_data),\n",
    "    batch_size=bs,device=device,shuffle=True,sort_within_batch=True)\n",
    "\n",
    "#测试\n",
    "for x in train_iterator:\n",
    "    print(x.text[0].shape)\n",
    "    print(x.text[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FrUp9HMEXWq6"
   },
   "source": [
    "sort_within_batch可以让iterator生成的batch按照长度排序，这是packed pad sequences所要求的。\n",
    "\n",
    "值得注意的是，**迭代器中的文本已经被转换成了序号**。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ti6yAeU0XWq7"
   },
   "source": [
    "## Model\n",
    "\n",
    "定义一个LSTM模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 838,
     "status": "ok",
     "timestamp": 1614415828817,
     "user": {
      "displayName": "义云",
      "photoUrl": "",
      "userId": "00944396917142299573"
     },
     "user_tz": -480
    },
    "id": "HKPyL0i3XWq7",
    "outputId": "69ca7256-329c-48b6-d1bb-4f82563e48d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple_rnn(\n",
      "  (embed): Embedding(244103, 100)\n",
      "  (rnn): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class simple_rnn(nn.Module):\n",
    "    \n",
    "    def __init__(self,d_vocab: int,d_embed:int ,d_hidden:int ,d_output:int,dropout=0,vectors=None,\n",
    "                 n_layers=1,bidirectional=False,pad_idx=0):\n",
    "        super(simple_rnn, self).__init__()\n",
    "        self.bi=2 if bidirectional else 1\n",
    "        self.n_layers=n_layers\n",
    "        self.pad_idx=pad_idx\n",
    "        self.d_hidden=d_hidden\n",
    "        self.d_output=d_output\n",
    "        \n",
    "        self.embed=nn.Embedding.from_pretrained(TEXT.vocab.vectors)\n",
    "        self.rnn=nn.LSTM(d_embed,d_hidden,batch_first=True,num_layers=n_layers,bidirectional=bidirectional,dropout=dropout)\n",
    "        self.fc=nn.Linear(d_hidden*self.bi,d_output)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,text,text_length):\n",
    "        # input:(bs,1ength),(bs)\n",
    "        #print(text.shape)\n",
    "        #print(text_length)\n",
    "        embeded=self.dropout(self.embed(text)) #(bs,length,d_embed)\n",
    "        packed=nn.utils.rnn.pack_padded_sequence(embeded,text_length,batch_first=True)\n",
    "        output,(hidden,cell)=self.rnn(packed)\n",
    "        output,output_len=nn.utils.rnn.pad_packed_sequence(output,batch_first=True)\n",
    "        #print(output)\n",
    "        #print(output_len)\n",
    "        output=torch.gather(output,1,(text_length-1).unsqueeze(-1).unsqueeze(-1).expand(-1,-1,self.d_hidden*self.bi))   #(bs,1,d_hidden*bi)\n",
    "        #print(output.shape)\n",
    "        #print(output)\n",
    "        '''\n",
    "        if self.bi==2:\n",
    "            hidden=torch.cat((hidden[-1,:,:],hidden[-2,:,:]),dim=1)\n",
    "        print(hidden)\n",
    "        '''\n",
    "\n",
    "        return self.fc(output.squeeze())#(batch,d_output)\n",
    "    \n",
    "model=simple_rnn(d_vocab,d_embed,d_hidden,d_output,dropout,n_layers=n_layers,bidirectional=bidirectional,pad_idx=TEXT.vocab.stoi[TEXT.pad_token])\n",
    "print(model)\n",
    "if use_cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6MHib9-XWq7"
   },
   "source": [
    "测试一下能否跑通"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 810,
     "status": "ok",
     "timestamp": 1614414685184,
     "user": {
      "displayName": "义云",
      "photoUrl": "",
      "userId": "00944396917142299573"
     },
     "user_tz": -480
    },
    "id": "Ftn25lupXWq8",
    "outputId": "dd7f182c-50e4-4174-f1c5-1f7e95872643"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if use_cuda:\n",
    "    criterion.cuda()\n",
    "with torch.no_grad():\n",
    "    for batch in train_iterator:\n",
    "        x,l=batch.text\n",
    "        y=batch.label\n",
    "        if use_cuda:\n",
    "            x.cuda()\n",
    "            y.cuda()\n",
    "            l.cuda()\n",
    "        preds=model(x,l)\n",
    "        print(preds.shape)\n",
    "        criterion(preds,y)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6iqL6TAXWq8"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 850,
     "status": "ok",
     "timestamp": 1614415014049,
     "user": {
      "displayName": "义云",
      "photoUrl": "",
      "userId": "00944396917142299573"
     },
     "user_tz": -480
    },
    "id": "lkcdocGlXWq8"
   },
   "outputs": [],
   "source": [
    "def train(model, train_iter, dev_iter, test_iter):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if use_cuda:\n",
    "        criterion.cuda()\n",
    "\n",
    "    # 学习率指数衰减，每次epoch：学习率 = gamma * 学习率\n",
    "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    dev_best_loss = float('inf')\n",
    "    last_improve = 0  # 记录上次验证集loss下降的batch数\n",
    "    #writer = SummaryWriter(log_dir=config.log_path + '/' + time.strftime('%m-%d.%H.%M', time.localtime())+'_'+which_data+'_'+which_model+'_'+which_task+'_'+exp_number)\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        train_loss=0\n",
    "        train_correct=0\n",
    "        # scheduler.step() # 学习率衰减\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            optimizer.zero_grad()\n",
    "            x=batch.text\n",
    "            y=batch.label\n",
    "            if use_cuda:\n",
    "                x.cuda()\n",
    "                y.cuda()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #训练集的准确率\n",
    "            true = y.data.cpu()\n",
    "            preds = torch.max(outputs.data, 1)[1].cpu()\n",
    "            train_correct+=(true==preds).sum()\n",
    "            train_loss+=loss.item()\n",
    "        train_loss/=len(train_iterator)   #train_loss\n",
    "        train_acc=train_correct/len(train_iterator.dataset)   #train_acc\n",
    "            \n",
    "        #验证集\n",
    "        dev_acc, dev_loss = evaluate(model, dev_iter)\n",
    "        if dev_loss < dev_best_loss:\n",
    "            dev_best_loss = dev_loss\n",
    "            improve = '*'\n",
    "            last_improve=epoch\n",
    "        else:\n",
    "            improve = ''\n",
    "        msg = 'Epoch: {0:>6},  Train Loss: {1:>5.2},  Train Acc: {2:>6.2%},  Val Loss: {3:>5.2},  Val Acc: {4:>6.2%} {5}'\n",
    "        print(msg.format(epoch+1, train_loss, train_acc, dev_loss, dev_acc, improve))\n",
    "        #writer.add_scalar(\"loss/train\", loss.item(), total_batch)\n",
    "        #writer.add_scalar(\"loss/dev\", dev_loss, total_batch)\n",
    "        #writer.add_scalar(\"acc/train\", train_acc, total_batch)\n",
    "        #writer.add_scalar(\"acc/dev\", dev_acc, total_batch)\n",
    "\n",
    "        if epoch - last_improve > require_improvement:\n",
    "            # 验证集loss超过1epoch没下降，结束训练\n",
    "            print(\"No optimization for a long time, auto-stopping...\")\n",
    "            break\n",
    "    #writer.close()\n",
    "    test(model, test_iter)\n",
    "\n",
    "def evaluate(model, data_iter, test=False):\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    with torch.no_grad():\n",
    "        for batch in data_iter:\n",
    "            x=batch.text\n",
    "            labels=batch.label\n",
    "            if use_cuda:\n",
    "                x.cuda()\n",
    "                labels.cuda()\n",
    "            outputs = model(x)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss_total += loss\n",
    "            labels = labels.data.cpu().numpy()\n",
    "            predic = torch.max(outputs.data, 1)[1].cpu().numpy()\n",
    "            labels_all = np.append(labels_all, labels)\n",
    "            predict_all = np.append(predict_all, predic)\n",
    "    model.train()\n",
    "    acc = metrics.accuracy_score(labels_all, predict_all)\n",
    "    \n",
    "    if test:\n",
    "        report = metrics.classification_report(labels_all, predict_all, labels=[0,1],target_names=['pos','neg'], digits=4,output_dict=True)\n",
    "        confusion = metrics.confusion_matrix(labels_all, predict_all)\n",
    "        return acc, loss_total / len(data_iter), report, confusion\n",
    "    \n",
    "    return acc, loss_total / len(data_iter)\n",
    "\n",
    "\n",
    "def test(model, test_iter):\n",
    "    test_acc, test_loss, test_report, test_confusion = evaluate(model, test_iter, test=True)\n",
    "    msg = 'Test Loss: {0:>5.2},  Test Acc: {1:>6.2%}'\n",
    "    print(msg.format(test_loss, test_acc))\n",
    "    print(\"Precision, Recall and F1-Score...\")\n",
    "    print(test_report)\n",
    "    print(\"Confusion Matrix...\")\n",
    "    print(test_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 124162,
     "status": "ok",
     "timestamp": 1614415140030,
     "user": {
      "displayName": "义云",
      "photoUrl": "",
      "userId": "00944396917142299573"
     },
     "user_tz": -480
    },
    "id": "T-MB93XvnWdP",
    "outputId": "b13e9af9-5bdf-4df9-96cb-40991981c939"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:      1,  Train Loss:  0.69,  Train Acc: 50.34%,  Val Loss:   0.7,  Val Acc: 50.94% *\n",
      "Epoch:      2,  Train Loss:  0.69,  Train Acc: 49.79%,  Val Loss:   0.7,  Val Acc: 52.42% *\n",
      "Epoch:      3,  Train Loss:  0.69,  Train Acc: 50.13%,  Val Loss:  0.71,  Val Acc: 51.92% \n",
      "Epoch:      4,  Train Loss:  0.69,  Train Acc: 50.41%,  Val Loss:  0.71,  Val Acc: 54.28% \n",
      "No optimization for a long time, auto-stopping...\n",
      "Test Loss:   0.8,  Test Acc: 53.72%\n",
      "Precision, Recall and F1-Score...\n",
      "{'pos': {'precision': 0.540250756593169, 'recall': 0.49984, 'f1-score': 0.5192603365884064, 'support': 12500}, 'neg': {'precision': 0.5346483066617045, 'recall': 0.57464, 'f1-score': 0.5539232697127434, 'support': 12500}, 'accuracy': 0.53724, 'macro avg': {'precision': 0.5374495316274368, 'recall': 0.53724, 'f1-score': 0.536591803150575, 'support': 25000}, 'weighted avg': {'precision': 0.5374495316274367, 'recall': 0.53724, 'f1-score': 0.536591803150575, 'support': 25000}}\n",
      "Confusion Matrix...\n",
      "[[6248 6252]\n",
      " [5317 7183]]\n"
     ]
    }
   ],
   "source": [
    "#lr=1e-3\n",
    "train(model,train_iterator,valid_iterator,test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 94561,
     "status": "ok",
     "timestamp": 1614415383819,
     "user": {
      "displayName": "义云",
      "photoUrl": "",
      "userId": "00944396917142299573"
     },
     "user_tz": -480
    },
    "id": "Kj2jzRtOpfa6",
    "outputId": "0a6af5cd-f6de-460f-a8c7-1691265188d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:      1,  Train Loss:  0.69,  Train Acc: 49.44%,  Val Loss:  0.69,  Val Acc: 49.88% *\n",
      "Epoch:      2,  Train Loss:  0.69,  Train Acc: 50.35%,  Val Loss:  0.69,  Val Acc: 50.76% \n",
      "Epoch:      3,  Train Loss:  0.69,  Train Acc: 50.18%,  Val Loss:  0.69,  Val Acc: 50.18% \n",
      "No optimization for a long time, auto-stopping...\n",
      "Test Loss:  0.69,  Test Acc: 50.06%\n",
      "Precision, Recall and F1-Score...\n",
      "{'pos': {'precision': 0.5018676627534685, 'recall': 0.15048, 'f1-score': 0.23153618906942391, 'support': 12500}, 'neg': {'precision': 0.5003293807641633, 'recall': 0.85064, 'f1-score': 0.6300663664375443, 'support': 12500}, 'accuracy': 0.50056, 'macro avg': {'precision': 0.501098521758816, 'recall': 0.50056, 'f1-score': 0.4308012777534841, 'support': 25000}, 'weighted avg': {'precision': 0.5010985217588159, 'recall': 0.50056, 'f1-score': 0.43080127775348415, 'support': 25000}}\n",
      "Confusion Matrix...\n",
      "[[ 1881 10619]\n",
      " [ 1867 10633]]\n"
     ]
    }
   ],
   "source": [
    "#lr=1e-4\n",
    "train(model,train_iterator,valid_iterator,test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 125278,
     "status": "ok",
     "timestamp": 1614415962115,
     "user": {
      "displayName": "义云",
      "photoUrl": "",
      "userId": "00944396917142299573"
     },
     "user_tz": -480
    },
    "id": "v6dE7x5trlR3",
    "outputId": "8105be3c-d930-4307-b5a0-cd3e53c8af8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:      1,  Train Loss:  0.69,  Train Acc: 49.61%,  Val Loss:  0.69,  Val Acc: 49.56% *\n",
      "Epoch:      2,  Train Loss:  0.69,  Train Acc: 50.21%,  Val Loss:  0.69,  Val Acc: 51.24% *\n",
      "Epoch:      3,  Train Loss:  0.69,  Train Acc: 49.29%,  Val Loss:   0.7,  Val Acc: 49.90% \n",
      "Epoch:      4,  Train Loss:  0.69,  Train Acc: 49.94%,  Val Loss:  0.69,  Val Acc: 50.38% \n",
      "No optimization for a long time, auto-stopping...\n",
      "Test Loss:  0.68,  Test Acc: 52.64%\n",
      "Precision, Recall and F1-Score...\n",
      "{'pos': {'precision': 0.5794933655006032, 'recall': 0.19216, 'f1-score': 0.2886151997596876, 'support': 12500}, 'neg': {'precision': 0.5157995684488133, 'recall': 0.86056, 'f1-score': 0.6450007495128167, 'support': 12500}, 'accuracy': 0.52636, 'macro avg': {'precision': 0.5476464669747082, 'recall': 0.5263599999999999, 'f1-score': 0.4668079746362521, 'support': 25000}, 'weighted avg': {'precision': 0.5476464669747082, 'recall': 0.52636, 'f1-score': 0.4668079746362521, 'support': 25000}}\n",
      "Confusion Matrix...\n",
      "[[ 2402 10098]\n",
      " [ 1743 10757]]\n"
     ]
    }
   ],
   "source": [
    "#dropout=0\n",
    "train(model,train_iterator,valid_iterator,test_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1VRwk9VjokWP"
   },
   "source": [
    "## Results and Analysis\n",
    "\n",
    "可以看到训练的效果相当差，这与所参照的博客一致，思考可能由以下原因导致：\n",
    "* 文本长度没有处理，有过长的文本\n",
    "* 没有使用pack pad等操作\n",
    "\n",
    "除此之外，还将在后续进行以下优化：\n",
    "* 使用预训练词向量\n",
    "* 调整dropout\n",
    "* 调整momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "IMDB_LSTM.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
