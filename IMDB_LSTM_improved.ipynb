{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"IMDB_LSTM_improved.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9K3I2xacXWqq"},"source":["# LSTM on IMDB Sentiment Analysis\n","\n","这个文件在IMDB上进行情感分类，练习torchtext等的使用，熟悉训练流程，并测试以下模型的表现：\n","\n","* LSTM\n","\n","相对前一个文件做了以下改进\n","\n","* 使用预训练词向量，比较不同维度词向量的影响\n","* 进行了pack，pad操作\n","* 考虑了num_layer,dropout等的影响\n","\n","目前的进度：\n","\n","* 完成，效果良好\n","\n","问题：\n","* BucketIterator，shuffle，packandpad这几个怎么处理？（目前采用的是只在batch间shuffle，长度相近的在同一个batch，batch内按降序排列）\n","* packpad操作之后hidden和output对应位置不一致？\n","* Adam不需要指定学习率？\n","* 实验中有很多超参等设置时是不是只能一个一个按当前最优选，还是严格控制变量做很多实验？\n","* 词典最大长度需要指定吗，怎么指定？停用词需要去除吗？\n","\n","\n","参考：\n","* [torchtext使用--updated IMDB](https://blog.csdn.net/weixin_43301333/article/details/105745053)"]},{"cell_type":"markdown","metadata":{"id":"atw_K4Opf9p-"},"source":["## Requirement\n","* torchtext==0.6.0"]},{"cell_type":"markdown","metadata":{"id":"aJSCPavXXWqz"},"source":["## Import"]},{"cell_type":"code","metadata":{"id":"dwiSrbYnXWq0","executionInfo":{"status":"ok","timestamp":1614677873334,"user_tz":-480,"elapsed":894,"user":{"displayName":"义云","photoUrl":"","userId":"00944396917142299573"}}},"source":["import torch\n","from torchtext import datasets\n","from torchtext import data\n","import numpy as np\n","import random\n","from torch import nn,optim\n","from sklearn import metrics\n","import torch.nn.functional as F\n","\n","use_cuda=torch.cuda.is_available()\n","device=torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","SEED = 1234\n","np.random.seed(SEED)\n","random.seed(SEED)\n","torch.manual_seed(SEED)\n","if use_cuda:\n","    torch.cuda.manual_seed(SEED)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HyFvA66KXWq0"},"source":["## 供调整的参数"]},{"cell_type":"code","metadata":{"id":"wS8cEXVvXWq1","executionInfo":{"status":"ok","timestamp":1614687584309,"user_tz":-480,"elapsed":759,"user":{"displayName":"义云","photoUrl":"","userId":"00944396917142299573"}}},"source":["bs=32\n","d_embed=100\n","d_hidden=256\n","d_output=2\n","dropout=0.5\n","max_epochs=20\n","require_improvement=3\n","n_layers=1\n","bidirectional=True\n","MAX_VOCAB_SIZE=25000"],"execution_count":91,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cr7-Kb3zXWq1"},"source":["## 数据载入和处理\n","\n","在载入和处理数据部分采用了torchtext库。\n","\n","由于在colab上无法运行spacy，我们采用简单的按空格分词，spacy后续在服务器上跑时加进去。"]},{"cell_type":"code","metadata":{"id":"1s72npWTXWq2","executionInfo":{"status":"ok","timestamp":1614686946695,"user_tz":-480,"elapsed":3744,"user":{"displayName":"义云","photoUrl":"","userId":"00944396917142299573"}}},"source":["tokenize = lambda x: x.split()\n","TEXT=data.Field(tokenize=tokenize,batch_first=True,include_lengths=True)\n","LABEL=data.LabelField(dtype=torch.long)\n","train_data,test_data=datasets.IMDB.splits(TEXT,LABEL)"],"execution_count":77,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y1AhFvm4XWq2"},"source":["**下面展示样本数量和一个样本。**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RcNJ6HMxXWq2","executionInfo":{"status":"ok","timestamp":1614686950146,"user_tz":-480,"elapsed":778,"user":{"displayName":"义云","photoUrl":"","userId":"00944396917142299573"}},"outputId":"5972bf7d-b184-4a9e-b099-f4d303220ca1"},"source":["print(f'Number of training examples: {len(train_data)}')\n","print(f'Number of testing examples: {len(test_data)}')\n","print(vars(train_data.examples[0])['text'])"],"execution_count":78,"outputs":[{"output_type":"stream","text":["Number of training examples: 25000\n","Number of testing examples: 25000\n","['At', 'one', 'end', 'of', 'the', 'Eighties', 'Warren', 'Beatty', 'created', 'and', 'starred', 'in', 'the', 'literate', 'epic', 'Reds', 'about', 'the', 'founding', 'of', 'the', 'Soviet', 'Union', 'as', 'seen', 'through', 'the', 'eyes', 'of', 'iconoclast', 'radical', 'John', 'Reed.', 'It', 'was', 'a', 'profound', 'film', 'both', 'entertaining', 'and', 'with', 'a', 'message', 'presented', 'by', 'an', 'all', 'star', 'cast.', 'At', 'the', 'end', 'of', 'the', 'decade', 'Warren', 'Beatty', 'created', 'another', 'kind', 'of', 'epic', 'in', 'Dick', 'Tracy', 'that', 'makes', 'no', 'pretense', 'to', 'being', 'anything', 'other', 'than', 'entertainment', 'with', 'a', 'whole', 'bunch', 'of', 'the', 'best', 'actors', 'around', 'just', 'having', 'a', 'great', 'old', 'time', 'hamming', 'it', 'up', 'under', 'tons', 'of', 'makeup.<br', '/><br', '/>That', 'both', 'Reds', 'and', 'Dick', 'Tracy', 'could', 'come', 'from', 'the', 'same', 'individual', 'speaks', 'volumes', 'about', 'the', 'range', 'this', 'man', 'has', 'as', 'a', 'player.', 'In', 'this', 'film', 'Beatty', 'managed', 'to', 'get', 'all', 'the', 'famous', 'cartoon', 'characters', 'from', 'the', 'strip', 'and', 'put', 'them', 'in', 'one', 'original', 'screenplay.<br', '/><br', '/>The', \"city's\", 'top', 'mobster', 'Big', 'Boy', 'Caprice', 'is', 'making', 'a', 'move', 'to', 'really', 'eliminate', 'competition.', 'The', 'film', 'opens', 'with', 'him', 'rubbing', 'out', 'Lips', \"Manlis's\", 'henchmen', 'in', 'a', 'Valentine', 'Massacre', 'style', 'shooting', 'and', 'then', 'Lips', 'himself', 'being', 'fitted', 'for', 'a', 'cement', 'overcoat.', 'But', \"Caprice's\", 'moves', 'are', 'making', 'him', 'a', 'target', 'for', 'Tracy.<br', '/><br', '/>In', 'the', 'meantime', 'a', 'third', 'mysterious', 'and', 'faceless', 'individual', 'is', 'looking', 'to', 'topple', 'Caprice', 'himself.', 'Will', 'our', 'hero', 'sort', 'out', 'this', 'thicket', 'of', 'crime?<br', '/><br', '/>The', 'spirit', 'of', 'fun', 'this', 'film', 'has', 'is', 'truly', 'infectious.', 'When', 'people', 'like', 'Al', 'Pacino,', 'Dustin', 'Hoffman,', 'Paul', 'Sorvino,', 'William', 'Forsythe,', 'R.G.', 'Armstrong', 'get', 'themselves', 'outrageously', 'made-up', 'to', 'look', 'like', 'the', 'cartoon', 'creations', 'of', 'strip', 'author', 'Chester', 'Gould', 'and', 'then', 'indulge', 'in', 'an', 'exercise', 'of', 'carving', 'the', 'biggest', 'slice', 'of', 'ham,', \"you've\", 'got', 'to', 'love', 'this', 'film.<br', '/><br', '/>Al', 'Pacino', 'got', 'a', 'nomination', 'for', 'Best', 'Supporting', 'Actor,', 'but', 'any', 'of', 'these', 'guys', 'could', 'have,', \"it's\", 'only', 'that', 'Pacino', 'as', 'Big', 'Boy', 'Caprice', 'gets', 'the', 'most', 'screen', 'time.', 'Only', 'Beatty', 'plays', 'it', 'completely', 'straight,', 'the', 'others', 'all', 'seem', 'to', 'play', 'off', 'of', 'him.', 'Dick', 'Tracy', 'won', 'Oscars', 'for', 'Best', 'Art&Set', 'Design,', 'Best', 'Song', 'written', 'by', 'Stephen', 'Sondheim', 'and', 'introduced', 'by', 'Madonna,', 'Sooner', 'Or', 'Later.', 'The', 'fact', 'he', 'was', 'even', 'able', 'to', 'get', 'somebody', 'like', 'Sondheim', 'to', 'write', 'a', 'score', 'for', 'this', 'film', 'only', 'shows', 'Sondheim', 'wanted', 'to', 'get', 'in', 'on', 'the', 'fun.', 'As', 'for', 'Madonna,', 'the', 'Material', 'Girl', 'does', 'more', 'than', 'hold', 'her', 'own', 'with', 'all', 'these', 'acting', 'heavyweights', 'as', 'club', 'torch', 'singer', 'Breathless', 'Mahoney.<br', '/><br', '/>Before', 'this', 'film,', 'Dick', 'Tracy', 'movies', 'were', 'consigned', 'to', 'the', 'B', 'pictures', 'and', 'worse', 'as', 'Saturday', 'afternoon', 'serials.', 'The', 'only', 'thing', 'that', 'rivals', 'this', 'all', 'star', 'extravaganza', 'is', 'a', 'radio', 'broadcast', 'done', 'for', 'Armed', 'Forces', 'Radio', 'during', 'World', 'War', 'II', 'that', 'got', 'to', 'vinyl.', 'Can', 'you', 'believe', 'a', 'cast', 'like', 'Bing', 'Crosby,', 'Bob', 'Hope,', 'Frank', 'Sinatra,', 'Dinah', 'Shore,', 'Jimmy', 'Durante,', 'Judy', 'Garland,', 'Frank', 'Morgan,', 'and', 'the', 'Andrews', 'Sisters?', 'Try', 'and', 'find', 'a', 'recording', 'of', 'that', 'gem.<br', '/><br', '/>Until', 'then', 'Warren', \"Beatty's\", 'classic', 'comic', 'strip', 'for', 'the', 'big', 'screen', 'will', 'do', 'nicely.']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"adpCmyEPXWq3"},"source":["有25000个训练样本和25000个测试样本，尽管这个数量比不太符合要求，但是这个任务比较简单，我们就这么来。\n","\n","一个样本是一个字典的形式，'text'中含有分词完毕的单词列表，'label'中含其标签（pos或neg）。\n","\n","**下面我们需要把训练样本中再分一些出来作为验证集。**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UAoPnEQdXWq4","executionInfo":{"status":"ok","timestamp":1614686953084,"user_tz":-480,"elapsed":870,"user":{"displayName":"义云","photoUrl":"","userId":"00944396917142299573"}},"outputId":"30b1c253-677d-4f51-b911-3697fa4b9025"},"source":["train_data,valid_data=train_data.split(split_ratio=0.8)\n","\n","print(f'Number of training examples: {len(train_data)}')\n","print(f'Number of validation examples: {len(valid_data)}')\n","print(f'Number of testing examples: {len(test_data)}')"],"execution_count":79,"outputs":[{"output_type":"stream","text":["Number of training examples: 20000\n","Number of validation examples: 5000\n","Number of testing examples: 25000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"puG1JBfGXWq4"},"source":["**下面我们需要建立词典**\n","\n","**这里我们使用Glove的100维词向量初始化**\n","\n","这里词典最大长度是否需要指定？"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WEefVjpaXWq5","executionInfo":{"status":"ok","timestamp":1614686960734,"user_tz":-480,"elapsed":3358,"user":{"displayName":"义云","photoUrl":"","userId":"00944396917142299573"}},"outputId":"90597981-3fd7-4e44-c665-490f1e198f28"},"source":["TEXT.build_vocab(train_data,vectors='glove.6B.100d',unk_init=torch.Tensor.normal_,max_size=MAX_VOCAB_SIZE)\n","LABEL.build_vocab(train_data)\n","\n","d_vocab=len(TEXT.vocab)\n","print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n","print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n","\n","print('最频繁的20个单词：')\n","print(TEXT.vocab.freqs.most_common(20))"],"execution_count":80,"outputs":[{"output_type":"stream","text":["Unique tokens in TEXT vocabulary: 25002\n","Unique tokens in LABEL vocabulary: 2\n","最频繁的20个单词：\n","[('the', 229686), ('a', 124475), ('and', 122205), ('of', 114632), ('to', 106377), ('is', 82794), ('in', 68497), ('I', 52823), ('that', 51928), ('this', 45800), ('it', 43628), ('/><br', 41010), ('was', 37436), ('as', 34034), ('with', 33323), ('for', 33022), ('but', 27233), ('The', 27013), ('on', 24694), ('movie', 24354)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7qFlMGk2XWq5"},"source":["测试和验证文本中可能出现训练集中没有的单词，另外在训练时为了满足批量输入需要将所有或一个批次的文本长度对齐，因此上述字典的建立中会自动加入特殊标记_&lt;unk&gt;_ 和*&lt;pad&gt;* ，用来表示未知字符和填充字符。\n","\n","\n","**下面我们需要建立迭代器**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vl5xshlgXWq6","executionInfo":{"status":"ok","timestamp":1614687594366,"user_tz":-480,"elapsed":743,"user":{"displayName":"义云","photoUrl":"","userId":"00944396917142299573"}},"outputId":"df26436a-40d6-485a-d0f6-038df8d8b9d9"},"source":["train_iterator, valid_iterator, test_iterator =data.BucketIterator.splits(\n","    (train_data,valid_data,test_data),\n","    batch_size=bs,device=device,sort_within_batch=True)\n","\n","#测试\n","for x in train_iterator:\n","    print(x.text[0].shape)\n","    print(x.text[0])\n","    print(x.label)\n","    break"],"execution_count":92,"outputs":[{"output_type":"stream","text":["torch.Size([32, 453])\n","tensor([[ 1973,     5,    28,  ...,    46,     5,  1340],\n","        [  133,  1999,   703,  ...,   946, 15968,  3272],\n","        [ 1675,    87,     9,  ...,    14,   872,     1],\n","        ...,\n","        [   49,    21,   154,  ...,     1,     1,     1],\n","        [   19,  6010,     5,  ...,     1,     1,     1],\n","        [  122,   166,     3,  ...,     1,     1,     1]], device='cuda:0')\n","tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n","        0, 1, 0, 1, 0, 1, 1, 1], device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FrUp9HMEXWq6"},"source":["sort_within_batch可以让iterator生成的batch按照长度排序，这是packed pad sequences所要求的。\n","\n","值得注意的是，**迭代器中的文本已经被转换成了序号**。\n"]},{"cell_type":"markdown","metadata":{"id":"ti6yAeU0XWq7"},"source":["## Model\n","\n","定义一个LSTM模型。\n","\n","对应之前使用预训练词向量，在这里我们使用nn.Embedding.from_pretrained。\n","\n","**非常值得注意的是，我们需要设置freeze=False，选择不冻结词向量，否则会影响收敛速度。**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HKPyL0i3XWq7","executionInfo":{"status":"ok","timestamp":1614687597558,"user_tz":-480,"elapsed":790,"user":{"displayName":"义云","photoUrl":"","userId":"00944396917142299573"}},"outputId":"ffcdbac9-4cb7-4902-bc5e-9578189c4522"},"source":["class simple_rnn(nn.Module):\n","    \n","    def __init__(self,d_vocab: int,d_embed:int ,d_hidden:int ,d_output:int,dropout=0,vectors=None,\n","                 n_layers=1,bidirectional=False,pad_idx=0):\n","        super(simple_rnn, self).__init__()\n","        self.bi=2 if bidirectional else 1\n","        self.n_layers=n_layers\n","        self.pad_idx=pad_idx\n","        self.d_hidden=d_hidden\n","        self.d_output=d_output\n","        \n","        self.embed=nn.Embedding.from_pretrained(TEXT.vocab.vectors,padding_idx=pad_idx,freeze=False)\n","        self.rnn=nn.LSTM(d_embed,d_hidden,batch_first=True,num_layers=n_layers,bidirectional=bidirectional,dropout=dropout)\n","        self.fc=nn.Linear(d_hidden*self.bi,d_output)\n","        self.dropout=nn.Dropout(dropout)\n","        \n","    def forward(self,text,text_length):\n","        # input:(bs,1ength),(bs)\n","        #print(text.shape)\n","        #print(text_length)\n","        embeded=self.dropout(self.embed(text)) #(bs,length,d_embed)\n","        packed=nn.utils.rnn.pack_padded_sequence(embeded,text_length.cpu(),batch_first=True)\n","        output,(hidden,cell)=self.rnn(packed)\n","        output,output_len=nn.utils.rnn.pad_packed_sequence(output,batch_first=True)\n","        #print(output)\n","        #print(output_len)\n","        #output=torch.gather(output,1,(text_length-1).unsqueeze(-1).unsqueeze(-1).expand(-1,-1,self.d_hidden*self.bi))   #(bs,1,d_hidden*bi)\n","        #print(output.shape)\n","        #print(output)\n","\n","        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n","\n","        return self.fc(hidden)#(batch,d_output)\n","    \n","model=simple_rnn(d_vocab,d_embed,d_hidden,d_output,dropout,n_layers=n_layers,bidirectional=bidirectional,pad_idx=TEXT.vocab.stoi[TEXT.pad_token])\n","print(model)\n","\n","PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n","UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n","model.embed.weight.data[UNK_IDX] = torch.zeros(d_embed)\n","model.embed.weight.data[PAD_IDX] = torch.zeros(d_embed)\n","print(model.embed.weight.data)"],"execution_count":93,"outputs":[{"output_type":"stream","text":["simple_rnn(\n","  (embed): Embedding(25002, 100, padding_idx=1)\n","  (rnn): LSTM(100, 256, batch_first=True, dropout=0.5, bidirectional=True)\n","  (fc): Linear(in_features=512, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",")\n","tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n","        ...,\n","        [-1.3295,  0.4554, -0.1147,  ...,  0.5420,  2.2063,  0.7768],\n","        [ 0.8933, -0.6821, -0.7016,  ...,  0.4941, -0.0156,  1.4400],\n","        [-0.1658,  0.3175, -0.3845,  ...,  0.9370, -0.3842, -2.0957]])\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"L6MHib9-XWq7"},"source":["测试一下能否跑通"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ftn25lupXWq8","executionInfo":{"status":"ok","timestamp":1614687365754,"user_tz":-480,"elapsed":816,"user":{"displayName":"义云","photoUrl":"","userId":"00944396917142299573"}},"outputId":"8e701a64-e0e9-4b73-df1a-c2779c859a97"},"source":["optimizer = optim.Adam(model.parameters())\n","criterion = nn.CrossEntropyLoss()\n","#criterion=nn.BCEWithLogitsLoss()\n","if use_cuda:\n","    criterion.to(device)\n","    model.to(device)\n","with torch.no_grad():\n","    for batch in train_iterator:\n","        x,l=batch.text\n","        y=batch.label\n","        preds=model(x,l)\n","        print(preds.shape)\n","        criterion(preds,y)\n","        break"],"execution_count":89,"outputs":[{"output_type":"stream","text":["torch.Size([128, 2])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"h6iqL6TAXWq8"},"source":["## Training\r\n","\r\n","因为是在colab上运行，没有保存模型，最后在测试集上的结果并非最优模型的结果。"]},{"cell_type":"code","metadata":{"id":"lkcdocGlXWq8","executionInfo":{"status":"ok","timestamp":1614687099544,"user_tz":-480,"elapsed":883,"user":{"displayName":"义云","photoUrl":"","userId":"00944396917142299573"}}},"source":["def train(model, train_iter, dev_iter, test_iter):\n","    model.to(device)\n","    model.train()\n","    optimizer = optim.Adam(model.parameters())\n","    criterion = nn.CrossEntropyLoss()\n","    #criterion = nn.BCEWithLogitsLoss()\n","    if use_cuda:\n","        criterion.cuda()\n","\n","    # 学习率指数衰减，每次epoch：学习率 = gamma * 学习率\n","    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n","    dev_best_loss = float('inf')\n","    last_improve = 0  # 记录上次验证集loss下降的batch数\n","    #writer = SummaryWriter(log_dir=config.log_path + '/' + time.strftime('%m-%d.%H.%M', time.localtime())+'_'+which_data+'_'+which_model+'_'+which_task+'_'+exp_number)\n","    \n","    for epoch in range(max_epochs):\n","        train_loss=0\n","        train_correct=0\n","        # scheduler.step() # 学习率衰减\n","        for i, batch in enumerate(train_iter):\n","            optimizer.zero_grad()\n","            x,l=batch.text\n","            y=batch.label\n","            outputs = model(x,l)\n","            loss = criterion(outputs, y)\n","            loss.backward()\n","            optimizer.step()\n","            #训练集的准确率\n","            preds = torch.max(outputs, 1)[1]\n","            #preds = torch.round(torch.sigmoid(outputs))\n","            train_correct+=(y==preds).sum()\n","            train_loss+=loss.item()\n","        train_loss/=len(train_iterator)   #train_loss\n","        train_acc=train_correct/len(train_iterator.dataset)   #train_acc\n","            \n","        #验证集\n","        dev_acc, dev_loss = evaluate(model, dev_iter)\n","        if dev_loss < dev_best_loss:\n","            dev_best_loss = dev_loss\n","            improve = '*'\n","            last_improve=epoch\n","        else:\n","            improve = ''\n","        msg = 'Epoch: {0:>6},  Train Loss: {1:>5.2},  Train Acc: {2:>6.2%},  Val Loss: {3:>5.2},  Val Acc: {4:>6.2%} {5}'\n","        print(msg.format(epoch+1, train_loss, train_acc, dev_loss, dev_acc, improve))\n","        #writer.add_scalar(\"loss/train\", loss.item(), total_batch)\n","        #writer.add_scalar(\"loss/dev\", dev_loss, total_batch)\n","        #writer.add_scalar(\"acc/train\", train_acc, total_batch)\n","        #writer.add_scalar(\"acc/dev\", dev_acc, total_batch)\n","\n","        if epoch - last_improve > require_improvement:\n","            # 验证集loss超过1epoch没下降，结束训练\n","            print(\"No optimization for a long time, auto-stopping...\")\n","            break\n","    #writer.close()\n","    test(model, test_iter)\n","\n","def evaluate(model, data_iter, test=False):\n","    model.eval()\n","    loss_total = 0\n","    predict_all = np.array([], dtype=int)\n","    labels_all = np.array([], dtype=int)\n","    with torch.no_grad():\n","        for batch in data_iter:\n","            x,l=batch.text\n","            labels=batch.label\n","            outputs = model(x,l)\n","            loss = F.cross_entropy(outputs, labels)\n","            #loss=criterion(outputs,labels)\n","            loss_total += loss\n","            labels = labels.data.cpu().numpy()\n","            predic = torch.max(outputs, 1)[1].cpu().numpy()\n","            #predic=torch.round(torch.sigmoid(outputs)).cpu().numpy()\n","            labels_all = np.append(labels_all, labels)\n","            predict_all = np.append(predict_all, predic)\n","    model.train()\n","    acc = metrics.accuracy_score(labels_all, predict_all)\n","    \n","    if test:\n","        report = metrics.classification_report(labels_all, predict_all, labels=[0,1],target_names=['pos','neg'], digits=4,output_dict=True)\n","        confusion = metrics.confusion_matrix(labels_all, predict_all)\n","        return acc, loss_total / len(data_iter), report, confusion\n","    \n","    return acc, loss_total / len(data_iter)\n","\n","\n","def test(model, test_iter):\n","    test_acc, test_loss, test_report, test_confusion = evaluate(model, test_iter, test=True)\n","    msg = 'Test Loss: {0:>5.2},  Test Acc: {1:>6.2%}'\n","    print(msg.format(test_loss, test_acc))\n","    print(\"Precision, Recall and F1-Score...\")\n","    print(test_report)\n","    print(\"Confusion Matrix...\")\n","    print(test_confusion)"],"execution_count":84,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T-MB93XvnWdP","executionInfo":{"status":"ok","timestamp":1614683712901,"user_tz":-480,"elapsed":614733,"user":{"displayName":"义云","photoUrl":"","userId":"00944396917142299573"}},"outputId":"ce0a435d-8f19-4111-c490-c247dedf8c77"},"source":["#original\n","train(model,train_iterator,valid_iterator,test_iterator)"],"execution_count":45,"outputs":[{"output_type":"stream","text":["Epoch:      1,  Train Loss:  0.64,  Train Acc: 62.28%,  Val Loss:  0.67,  Val Acc: 62.84% *\n","Epoch:      2,  Train Loss:  0.62,  Train Acc: 66.15%,  Val Loss:  0.59,  Val Acc: 70.22% *\n","Epoch:      3,  Train Loss:   0.6,  Train Acc: 67.28%,  Val Loss:  0.59,  Val Acc: 67.66% \n","Epoch:      4,  Train Loss:   0.5,  Train Acc: 76.34%,  Val Loss:  0.44,  Val Acc: 79.78% *\n","Epoch:      5,  Train Loss:  0.38,  Train Acc: 83.55%,  Val Loss:  0.33,  Val Acc: 85.82% *\n","Epoch:      6,  Train Loss:  0.31,  Train Acc: 86.75%,  Val Loss:  0.37,  Val Acc: 84.68% \n","Epoch:      7,  Train Loss:  0.28,  Train Acc: 88.79%,  Val Loss:   0.3,  Val Acc: 87.46% *\n","Epoch:      8,  Train Loss:  0.23,  Train Acc: 91.03%,  Val Loss:  0.28,  Val Acc: 88.38% *\n","Epoch:      9,  Train Loss:  0.21,  Train Acc: 91.81%,  Val Loss:  0.28,  Val Acc: 88.92% *\n","Epoch:     10,  Train Loss:  0.19,  Train Acc: 92.61%,  Val Loss:   0.3,  Val Acc: 89.00% \n","Epoch:     11,  Train Loss:  0.16,  Train Acc: 93.80%,  Val Loss:  0.32,  Val Acc: 88.70% \n","Epoch:     12,  Train Loss:  0.15,  Train Acc: 94.07%,  Val Loss:  0.31,  Val Acc: 88.96% \n","Epoch:     13,  Train Loss:  0.13,  Train Acc: 94.92%,  Val Loss:  0.29,  Val Acc: 89.28% \n","Epoch:     14,  Train Loss:  0.12,  Train Acc: 95.51%,  Val Loss:   0.3,  Val Acc: 89.40% \n","Epoch:     15,  Train Loss:   0.1,  Train Acc: 96.11%,  Val Loss:  0.33,  Val Acc: 89.14% \n","No optimization for a long time, auto-stopping...\n","Test Loss:  0.34,  Test Acc: 88.55%\n","Precision, Recall and F1-Score...\n","{'pos': {'precision': 0.8697820748925721, 'recall': 0.9068, 'f1-score': 0.8879053736487545, 'support': 12500}, 'neg': {'precision': 0.9026570855614974, 'recall': 0.86424, 'f1-score': 0.883030897498774, 'support': 12500}, 'accuracy': 0.88552, 'macro avg': {'precision': 0.8862195802270347, 'recall': 0.8855200000000001, 'f1-score': 0.8854681355737642, 'support': 25000}, 'weighted avg': {'precision': 0.8862195802270347, 'recall': 0.88552, 'f1-score': 0.8854681355737642, 'support': 25000}}\n","Confusion Matrix...\n","[[11335  1165]\n"," [ 1697 10803]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Y-_16HoIo01k"},"source":["## Ablation Studies"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kj2jzRtOpfa6","executionInfo":{"status":"ok","timestamp":1614684435903,"user_tz":-480,"elapsed":525326,"user":{"displayName":"义云","photoUrl":"","userId":"00944396917142299573"}},"outputId":"c2145c1b-5fa9-4c47-c2a3-7fe252994f19"},"source":["#词典没有最大长度限制\n","train(model,train_iterator,valid_iterator,test_iterator)"],"execution_count":49,"outputs":[{"output_type":"stream","text":["Epoch:      1,  Train Loss:  0.68,  Train Acc: 56.34%,  Val Loss:   0.7,  Val Acc: 50.78% *\n","Epoch:      2,  Train Loss:  0.68,  Train Acc: 54.80%,  Val Loss:  0.65,  Val Acc: 63.44% *\n","Epoch:      3,  Train Loss:  0.65,  Train Acc: 61.18%,  Val Loss:  0.53,  Val Acc: 74.84% *\n","Epoch:      4,  Train Loss:  0.45,  Train Acc: 79.60%,  Val Loss:  0.35,  Val Acc: 85.00% *\n","Epoch:      5,  Train Loss:   0.3,  Train Acc: 87.40%,  Val Loss:  0.32,  Val Acc: 86.62% *\n","Epoch:      6,  Train Loss:  0.23,  Train Acc: 91.01%,  Val Loss:   0.3,  Val Acc: 88.30% *\n","Epoch:      7,  Train Loss:  0.17,  Train Acc: 93.39%,  Val Loss:  0.31,  Val Acc: 87.76% \n","Epoch:      8,  Train Loss:  0.14,  Train Acc: 94.69%,  Val Loss:   0.4,  Val Acc: 86.84% \n","Epoch:      9,  Train Loss:  0.11,  Train Acc: 95.93%,  Val Loss:  0.38,  Val Acc: 87.20% \n","Epoch:     10,  Train Loss:  0.09,  Train Acc: 96.79%,  Val Loss:  0.38,  Val Acc: 88.28% \n","Epoch:     11,  Train Loss: 0.074,  Train Acc: 97.46%,  Val Loss:  0.38,  Val Acc: 88.08% \n","Epoch:     12,  Train Loss: 0.064,  Train Acc: 97.70%,  Val Loss:  0.39,  Val Acc: 88.30% \n","No optimization for a long time, auto-stopping...\n","Test Loss:  0.42,  Test Acc: 87.39%\n","Precision, Recall and F1-Score...\n","{'pos': {'precision': 0.8965634280865507, 'recall': 0.84528, 'f1-score': 0.8701667696108709, 'support': 12500}, 'neg': {'precision': 0.8536511539916761, 'recall': 0.90248, 'f1-score': 0.8773867392572428, 'support': 12500}, 'accuracy': 0.87388, 'macro avg': {'precision': 0.8751072910391133, 'recall': 0.87388, 'f1-score': 0.8737767544340569, 'support': 25000}, 'weighted avg': {'precision': 0.8751072910391134, 'recall': 0.87388, 'f1-score': 0.8737767544340569, 'support': 25000}}\n","Confusion Matrix...\n","[[10566  1934]\n"," [ 1219 11281]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RZIfnrhTrLg3"},"source":["略有下降，没有明显差异。\r\n","\r\n","在理想情况下，模型应该可以学习到哪些有意义的哪些是无意义的。而且频繁的不一定有价值（停用词），罕见的不一定无价值。这个问题还需深挖。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v6dE7x5trlR3","executionInfo":{"status":"ok","timestamp":1614685331202,"user_tz":-480,"elapsed":173295,"user":{"displayName":"义云","photoUrl":"","userId":"00944396917142299573"}},"outputId":"03835773-bfab-4d27-967a-28e8677e9f26"},"source":["#dropout=0\n","train(model,train_iterator,valid_iterator,test_iterator)"],"execution_count":56,"outputs":[{"output_type":"stream","text":["Epoch:      1,  Train Loss:  0.65,  Train Acc: 61.43%,  Val Loss:  0.56,  Val Acc: 71.30% *\n","Epoch:      2,  Train Loss:   0.5,  Train Acc: 75.11%,  Val Loss:  0.44,  Val Acc: 80.16% *\n","Epoch:      3,  Train Loss:  0.37,  Train Acc: 84.13%,  Val Loss:  0.39,  Val Acc: 83.44% *\n","Epoch:      4,  Train Loss:  0.25,  Train Acc: 89.73%,  Val Loss:  0.34,  Val Acc: 85.86% *\n","Epoch:      5,  Train Loss:  0.17,  Train Acc: 93.65%,  Val Loss:  0.33,  Val Acc: 87.18% *\n","Epoch:      6,  Train Loss:  0.11,  Train Acc: 95.97%,  Val Loss:  0.35,  Val Acc: 88.08% \n","Epoch:      7,  Train Loss: 0.071,  Train Acc: 97.73%,  Val Loss:  0.43,  Val Acc: 87.90% \n","Epoch:      8,  Train Loss: 0.045,  Train Acc: 98.56%,  Val Loss:  0.55,  Val Acc: 87.20% \n","Epoch:      9,  Train Loss: 0.029,  Train Acc: 99.09%,  Val Loss:  0.57,  Val Acc: 86.86% \n","No optimization for a long time, auto-stopping...\n","Test Loss:  0.62,  Test Acc: 85.63%\n","Precision, Recall and F1-Score...\n","{'pos': {'precision': 0.8401435881768884, 'recall': 0.88, 'f1-score': 0.8596100496229439, 'support': 12500}, 'neg': {'precision': 0.874023683547493, 'recall': 0.83256, 'f1-score': 0.852788134551563, 'support': 12500}, 'accuracy': 0.85628, 'macro avg': {'precision': 0.8570836358621907, 'recall': 0.8562799999999999, 'f1-score': 0.8561990920872534, 'support': 25000}, 'weighted avg': {'precision': 0.8570836358621907, 'recall': 0.85628, 'f1-score': 0.8561990920872535, 'support': 25000}}\n","Confusion Matrix...\n","[[11000  1500]\n"," [ 2093 10407]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fZVOleG3wDTR"},"source":["收敛变快，但是最终效果下降比较明显"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YCcCUHQRwO75","executionInfo":{"status":"ok","timestamp":1614685749449,"user_tz":-480,"elapsed":203038,"user":{"displayName":"义云","photoUrl":"","userId":"00944396917142299573"}},"outputId":"edc769b1-7211-4531-a233-ea18e3b824c5"},"source":["#layer=1\r\n","train(model,train_iterator,valid_iterator,test_iterator)"],"execution_count":59,"outputs":[{"output_type":"stream","text":["Epoch:      1,  Train Loss:  0.67,  Train Acc: 59.28%,  Val Loss:  0.61,  Val Acc: 65.86% *\n","Epoch:      2,  Train Loss:  0.61,  Train Acc: 65.82%,  Val Loss:   0.5,  Val Acc: 76.08% *\n","Epoch:      3,  Train Loss:  0.48,  Train Acc: 77.40%,  Val Loss:  0.44,  Val Acc: 81.00% *\n","Epoch:      4,  Train Loss:  0.37,  Train Acc: 83.57%,  Val Loss:  0.37,  Val Acc: 84.86% *\n","Epoch:      5,  Train Loss:  0.35,  Train Acc: 85.04%,  Val Loss:  0.31,  Val Acc: 87.48% *\n","Epoch:      6,  Train Loss:  0.28,  Train Acc: 88.73%,  Val Loss:   0.3,  Val Acc: 87.80% *\n","Epoch:      7,  Train Loss:  0.23,  Train Acc: 90.88%,  Val Loss:  0.29,  Val Acc: 87.98% *\n","Epoch:      8,  Train Loss:   0.2,  Train Acc: 91.97%,  Val Loss:  0.35,  Val Acc: 88.62% \n","Epoch:      9,  Train Loss:  0.18,  Train Acc: 92.92%,  Val Loss:   0.3,  Val Acc: 89.04% \n","Epoch:     10,  Train Loss:  0.17,  Train Acc: 93.69%,  Val Loss:  0.29,  Val Acc: 89.06% *\n","Epoch:     11,  Train Loss:  0.14,  Train Acc: 94.57%,  Val Loss:  0.38,  Val Acc: 85.14% \n","Epoch:     12,  Train Loss:  0.13,  Train Acc: 95.00%,  Val Loss:  0.33,  Val Acc: 89.02% \n","Epoch:     13,  Train Loss:  0.11,  Train Acc: 95.82%,  Val Loss:  0.34,  Val Acc: 89.08% \n","Epoch:     14,  Train Loss:   0.1,  Train Acc: 96.11%,  Val Loss:  0.32,  Val Acc: 88.96% \n","No optimization for a long time, auto-stopping...\n","Test Loss:  0.35,  Test Acc: 88.39%\n","Precision, Recall and F1-Score...\n","{'pos': {'precision': 0.8650437428680107, 'recall': 0.90968, 'f1-score': 0.8868005459153832, 'support': 12500}, 'neg': {'precision': 0.9047659215520877, 'recall': 0.85808, 'f1-score': 0.8808047628823651, 'support': 12500}, 'accuracy': 0.88388, 'macro avg': {'precision': 0.8849048322100492, 'recall': 0.88388, 'f1-score': 0.8838026543988742, 'support': 25000}, 'weighted avg': {'precision': 0.8849048322100492, 'recall': 0.88388, 'f1-score': 0.8838026543988742, 'support': 25000}}\n","Confusion Matrix...\n","[[11371  1129]\n"," [ 1774 10726]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7BJn-Si8wtum"},"source":["训练速度变快，而且最终效果没有下降。可能这个任务不需要深层理解，一层足以拟合，多层反而会过拟合。后续我们就采用一层。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yKfCDcg1xmjh","executionInfo":{"status":"ok","timestamp":1614686773085,"user_tz":-480,"elapsed":185968,"user":{"displayName":"义云","photoUrl":"","userId":"00944396917142299573"}},"outputId":"4d49100a-8e12-4870-b797-d8f65bdc9d3e"},"source":["#300维词向量\r\n","train(model,train_iterator,valid_iterator,test_iterator)"],"execution_count":70,"outputs":[{"output_type":"stream","text":["Epoch:      1,  Train Loss:  0.66,  Train Acc: 60.86%,  Val Loss:  0.64,  Val Acc: 64.22% *\n","Epoch:      2,  Train Loss:  0.57,  Train Acc: 70.24%,  Val Loss:  0.53,  Val Acc: 73.10% *\n","Epoch:      3,  Train Loss:  0.52,  Train Acc: 74.70%,  Val Loss:  0.48,  Val Acc: 78.32% *\n","Epoch:      4,  Train Loss:  0.48,  Train Acc: 77.45%,  Val Loss:  0.37,  Val Acc: 84.72% *\n","Epoch:      5,  Train Loss:  0.37,  Train Acc: 83.35%,  Val Loss:  0.34,  Val Acc: 86.76% *\n","Epoch:      6,  Train Loss:  0.25,  Train Acc: 90.18%,  Val Loss:  0.32,  Val Acc: 87.36% *\n","Epoch:      7,  Train Loss:  0.19,  Train Acc: 92.47%,  Val Loss:  0.34,  Val Acc: 87.92% \n","Epoch:      8,  Train Loss:  0.15,  Train Acc: 94.13%,  Val Loss:  0.33,  Val Acc: 88.60% \n","Epoch:      9,  Train Loss:  0.12,  Train Acc: 95.47%,  Val Loss:  0.39,  Val Acc: 88.24% \n","Epoch:     10,  Train Loss:   0.1,  Train Acc: 96.27%,  Val Loss:  0.37,  Val Acc: 88.42% \n","No optimization for a long time, auto-stopping...\n","Test Loss:  0.39,  Test Acc: 87.70%\n","Precision, Recall and F1-Score...\n","{'pos': {'precision': 0.8830894308943089, 'recall': 0.86896, 'f1-score': 0.8759677419354838, 'support': 12500}, 'neg': {'precision': 0.871023622047244, 'recall': 0.88496, 'f1-score': 0.8779365079365078, 'support': 12500}, 'accuracy': 0.87696, 'macro avg': {'precision': 0.8770565264707765, 'recall': 0.87696, 'f1-score': 0.8769521249359957, 'support': 25000}, 'weighted avg': {'precision': 0.8770565264707765, 'recall': 0.87696, 'f1-score': 0.8769521249359958, 'support': 25000}}\n","Confusion Matrix...\n","[[10862  1638]\n"," [ 1438 11062]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3Xl8va65y7tb"},"source":["结果和解释同上。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ul1xuRYczTO3","executionInfo":{"status":"ok","timestamp":1614687279464,"user_tz":-480,"elapsed":159423,"user":{"displayName":"义云","photoUrl":"","userId":"00944396917142299573"}},"outputId":"517d6e43-81e5-4de9-9c00-0fdeea1c86f8"},"source":["#CrossEntropy\r\n","train(model,train_iterator,valid_iterator,test_iterator)"],"execution_count":85,"outputs":[{"output_type":"stream","text":["Epoch:      1,  Train Loss:  0.67,  Train Acc: 58.53%,  Val Loss:  0.59,  Val Acc: 69.10% *\n","Epoch:      2,  Train Loss:  0.61,  Train Acc: 66.71%,  Val Loss:  0.49,  Val Acc: 77.38% *\n","Epoch:      3,  Train Loss:  0.48,  Train Acc: 77.37%,  Val Loss:  0.39,  Val Acc: 83.60% *\n","Epoch:      4,  Train Loss:  0.37,  Train Acc: 84.08%,  Val Loss:  0.33,  Val Acc: 85.98% *\n","Epoch:      5,  Train Loss:  0.29,  Train Acc: 87.89%,  Val Loss:  0.31,  Val Acc: 87.12% *\n","Epoch:      6,  Train Loss:  0.26,  Train Acc: 89.48%,  Val Loss:   0.3,  Val Acc: 87.56% *\n","Epoch:      7,  Train Loss:  0.22,  Train Acc: 91.28%,  Val Loss:  0.29,  Val Acc: 88.60% *\n","Epoch:      8,  Train Loss:   0.2,  Train Acc: 92.33%,  Val Loss:   0.3,  Val Acc: 88.50% \n","Epoch:      9,  Train Loss:  0.17,  Train Acc: 93.61%,  Val Loss:  0.31,  Val Acc: 88.62% \n","Epoch:     10,  Train Loss:  0.16,  Train Acc: 94.08%,  Val Loss:  0.33,  Val Acc: 89.40% \n","Epoch:     11,  Train Loss:  0.14,  Train Acc: 94.57%,  Val Loss:  0.31,  Val Acc: 89.30% \n","No optimization for a long time, auto-stopping...\n","Test Loss:  0.32,  Test Acc: 88.68%\n","Precision, Recall and F1-Score...\n","{'pos': {'precision': 0.8843494157857086, 'recall': 0.89008, 'f1-score': 0.8872054543279773, 'support': 12500}, 'neg': {'precision': 0.8893630727111683, 'recall': 0.8836, 'f1-score': 0.88647216983025, 'support': 12500}, 'accuracy': 0.88684, 'macro avg': {'precision': 0.8868562442484385, 'recall': 0.8868400000000001, 'f1-score': 0.8868388120791136, 'support': 25000}, 'weighted avg': {'precision': 0.8868562442484385, 'recall': 0.88684, 'f1-score': 0.8868388120791135, 'support': 25000}}\n","Confusion Matrix...\n","[[11126  1374]\n"," [ 1455 11045]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"daKxrda03Jbc"},"source":["果然是差不多的"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ua8Jonci3SKh","executionInfo":{"status":"ok","timestamp":1614687560618,"user_tz":-480,"elapsed":181118,"user":{"displayName":"义云","photoUrl":"","userId":"00944396917142299573"}},"outputId":"06e7f2b8-0d48-4edd-a2e2-0ef48ad4f199"},"source":["#bs=128\r\n","train(model,train_iterator,valid_iterator,test_iterator)"],"execution_count":90,"outputs":[{"output_type":"stream","text":["Epoch:      1,  Train Loss:  0.67,  Train Acc: 57.86%,  Val Loss:  0.62,  Val Acc: 66.66% *\n","Epoch:      2,  Train Loss:  0.62,  Train Acc: 66.60%,  Val Loss:  0.69,  Val Acc: 51.74% \n","Epoch:      3,  Train Loss:  0.64,  Train Acc: 63.05%,  Val Loss:  0.53,  Val Acc: 74.16% *\n","Epoch:      4,  Train Loss:  0.65,  Train Acc: 61.83%,  Val Loss:  0.65,  Val Acc: 63.94% \n","Epoch:      5,  Train Loss:  0.61,  Train Acc: 67.12%,  Val Loss:  0.56,  Val Acc: 72.78% \n","Epoch:      6,  Train Loss:  0.48,  Train Acc: 78.21%,  Val Loss:  0.38,  Val Acc: 83.26% *\n","Epoch:      7,  Train Loss:  0.36,  Train Acc: 84.25%,  Val Loss:  0.34,  Val Acc: 84.80% *\n","Epoch:      8,  Train Loss:  0.31,  Train Acc: 87.05%,  Val Loss:  0.33,  Val Acc: 85.62% *\n","Epoch:      9,  Train Loss:  0.28,  Train Acc: 88.77%,  Val Loss:   0.3,  Val Acc: 87.62% *\n","Epoch:     10,  Train Loss:  0.26,  Train Acc: 89.76%,  Val Loss:  0.31,  Val Acc: 87.26% \n","Epoch:     11,  Train Loss:  0.23,  Train Acc: 90.92%,  Val Loss:  0.29,  Val Acc: 87.94% *\n","Epoch:     12,  Train Loss:  0.21,  Train Acc: 91.71%,  Val Loss:   0.3,  Val Acc: 86.84% \n","Epoch:     13,  Train Loss:  0.19,  Train Acc: 92.65%,  Val Loss:   0.3,  Val Acc: 88.32% \n","Epoch:     14,  Train Loss:  0.18,  Train Acc: 93.19%,  Val Loss:   0.3,  Val Acc: 88.38% \n","Epoch:     15,  Train Loss:  0.17,  Train Acc: 93.84%,  Val Loss:  0.29,  Val Acc: 88.36% \n","No optimization for a long time, auto-stopping...\n","Test Loss:  0.32,  Test Acc: 87.90%\n","Precision, Recall and F1-Score...\n","{'pos': {'precision': 0.8795064893446563, 'recall': 0.87824, 'f1-score': 0.8788727884076534, 'support': 12500}, 'neg': {'precision': 0.8784150822815147, 'recall': 0.87968, 'f1-score': 0.8790470860980094, 'support': 12500}, 'accuracy': 0.87896, 'macro avg': {'precision': 0.8789607858130855, 'recall': 0.87896, 'f1-score': 0.8789599372528314, 'support': 25000}, 'weighted avg': {'precision': 0.8789607858130853, 'recall': 0.87896, 'f1-score': 0.8789599372528314, 'support': 25000}}\n","Confusion Matrix...\n","[[10978  1522]\n"," [ 1504 10996]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cqI8qfXH4LQ2"},"source":["没有上升"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tcsxRqMI4NoV","executionInfo":{"status":"ok","timestamp":1614687827764,"user_tz":-480,"elapsed":224274,"user":{"displayName":"义云","photoUrl":"","userId":"00944396917142299573"}},"outputId":"4697a37c-de9e-4598-f43f-d50a695fa440"},"source":["#bs=32\r\n","train(model,train_iterator,valid_iterator,test_iterator)"],"execution_count":94,"outputs":[{"output_type":"stream","text":["Epoch:      1,  Train Loss:  0.68,  Train Acc: 56.91%,  Val Loss:   0.6,  Val Acc: 65.48% *\n","Epoch:      2,  Train Loss:   0.6,  Train Acc: 67.45%,  Val Loss:  0.57,  Val Acc: 70.26% *\n","Epoch:      3,  Train Loss:  0.44,  Train Acc: 79.86%,  Val Loss:  0.35,  Val Acc: 85.52% *\n","Epoch:      4,  Train Loss:  0.32,  Train Acc: 86.63%,  Val Loss:  0.31,  Val Acc: 86.96% *\n","Epoch:      5,  Train Loss:  0.27,  Train Acc: 89.08%,  Val Loss:  0.28,  Val Acc: 88.22% *\n","Epoch:      6,  Train Loss:  0.22,  Train Acc: 91.35%,  Val Loss:  0.29,  Val Acc: 88.54% \n","Epoch:      7,  Train Loss:  0.19,  Train Acc: 92.83%,  Val Loss:  0.29,  Val Acc: 89.04% \n","Epoch:      8,  Train Loss:  0.16,  Train Acc: 93.76%,  Val Loss:  0.28,  Val Acc: 89.12% *\n","Epoch:      9,  Train Loss:  0.14,  Train Acc: 94.72%,  Val Loss:  0.36,  Val Acc: 89.04% \n","Epoch:     10,  Train Loss:  0.13,  Train Acc: 95.11%,  Val Loss:   0.3,  Val Acc: 89.58% \n","Epoch:     11,  Train Loss:  0.11,  Train Acc: 96.14%,  Val Loss:  0.33,  Val Acc: 89.46% \n","Epoch:     12,  Train Loss:  0.09,  Train Acc: 96.71%,  Val Loss:  0.42,  Val Acc: 89.14% \n","No optimization for a long time, auto-stopping...\n","Test Loss:  0.44,  Test Acc: 88.46%\n","Precision, Recall and F1-Score...\n","{'pos': {'precision': 0.8897988971780733, 'recall': 0.87784, 'f1-score': 0.8837789948453607, 'support': 12500}, 'neg': {'precision': 0.8794600568361225, 'recall': 0.89128, 'f1-score': 0.8853305785123967, 'support': 12500}, 'accuracy': 0.88456, 'macro avg': {'precision': 0.8846294770070979, 'recall': 0.88456, 'f1-score': 0.8845547866788788, 'support': 25000}, 'weighted avg': {'precision': 0.8846294770070979, 'recall': 0.88456, 'f1-score': 0.8845547866788788, 'support': 25000}}\n","Confusion Matrix...\n","[[10973  1527]\n"," [ 1359 11141]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"msirrbnU5V0P"},"source":["几乎相同"]},{"cell_type":"markdown","metadata":{"id":"1VRwk9VjokWP"},"source":["## Results and Analysis\n","\n","训练效果良好，达到了正常水平，与[Benchmark](https://paperswithcode.com/sota/sentiment-analysis-on-imdb)中的结果接近。\n","\n","也比较了一些参数的设置，得到了一些有益的结论。\n","\n","之后将要进行的工作：\n","* 使用BERT在IMDB上文本分类\n","* 完善框架，保存模型，用正确的模型在测试集上测试"]}]}