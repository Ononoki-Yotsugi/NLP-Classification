{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Sentiment Analysis\n",
    "\n",
    "这个文件在IMDB上进行情感分类，测试以下模型的表现：\n",
    "\n",
    "* 一些机器学习方法（not finished）\n",
    "* LSTM\n",
    "* BERT（not finished）\n",
    "\n",
    "目前的进度：\n",
    "\n",
    "处理数据\n",
    "\n",
    "问题：\n",
    "\n",
    "* 迭代器并没有shuffle?\n",
    "* torchtext怎么使用预训练词向量？\n",
    "* RNN训练时要使用packandpad吗？还是仅仅pad就行？使用了之后还需要让长度相近的在一起pad吗？\n",
    "* 后续使用BERT可能要用transformer或者allennlp？\n",
    "* 训练过程中是按batch评估还是按epoch评估找最优？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import datasets\n",
    "from torchtext import data\n",
    "import numpy as np\n",
    "import random\n",
    "from torch import nn,optim\n",
    "from sklearn import metrics\n",
    "\n",
    "use_cuda=torch.cuda.is_available()\n",
    "device=torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 供调整的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-3\n",
    "bs=64\n",
    "d_embed=100\n",
    "d_hidden=256\n",
    "d_output=2\n",
    "dropout=0.2\n",
    "max_epochs=20\n",
    "require_improvement=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据载入和处理\n",
    "\n",
    "在载入和处理数据部分采用了torchtext库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT=data.Field(tokenize='spacy',tokenizer_language=\"en_core_web_sm\",batch_first=True)\n",
    "LABEL=data.LabelField(dtype=torch.long)\n",
    "train_data,test_data=datasets.IMDB.splits(TEXT,LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里可能会遇到报错，提示不能载入*en_core_web_sm*。\n",
    "\n",
    "输入命令（需要翻墙）即可解决\n",
    "> python -m spacy download en_core_web_sm\n",
    "\n",
    "**下面展示样本数量和一个样本。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 25000\n",
      "Number of testing examples: 25000\n",
      "['Bromwell', 'High', 'is', 'a', 'cartoon', 'comedy', '.', 'It', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', ',', 'such', 'as', '\"', 'Teachers', '\"', '.', 'My', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'Bromwell', 'High', \"'s\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '\"', 'Teachers', '\"', '.', 'The', 'scramble', 'to', 'survive', 'financially', ',', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', \"'\", 'pomp', ',', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', ',', 'all', 'remind', 'me', 'of', 'the', 'schools', 'I', 'knew', 'and', 'their', 'students', '.', 'When', 'I', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', ',', 'I', 'immediately', 'recalled', '.........', 'at', '..........', 'High', '.', 'A', 'classic', 'line', ':', 'INSPECTOR', ':', 'I', \"'m\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', '.', 'STUDENT', ':', 'Welcome', 'to', 'Bromwell', 'High', '.', 'I', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'Bromwell', 'High', 'is', 'far', 'fetched', '.', 'What', 'a', 'pity', 'that', 'it', 'is', \"n't\", '!']\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')\n",
    "print(vars(train_data.examples[0])['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有25000个训练样本和25000个测试样本，尽管这个数量比不太符合要求，但是这个任务比较简单，我们就这么来。\n",
    "\n",
    "一个样本是一个字典的形式，'text'中含有分词完毕的单词列表，'label'中含其标签（pos或neg）。\n",
    "\n",
    "**下面我们需要把训练样本中再分一些出来作为验证集。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 20000\n",
      "Number of validation examples: 5000\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "#确保每次分割相同\n",
    "SEED = 1234\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    \n",
    "train_data,valid_data=train_data.split(split_ratio=0.8)\n",
    "\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**下面我们需要建立字典**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 108284\n",
      "Unique tokens in LABEL vocabulary: 2\n",
      "最频繁的20个单词：\n",
      "[('the', 232320), (',', 219824), ('.', 189598), ('and', 125133), ('a', 124608), ('of', 115066), ('to', 107184), ('is', 87278), ('in', 70017), ('I', 61811), ('it', 61383), ('that', 56358), ('\"', 50570), (\"'s\", 49484), ('this', 48454), ('-', 42677), ('/><br', 40779), ('was', 40082), ('as', 34777), ('with', 34057)]\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_data)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "d_vocab=len(TEXT.vocab)\n",
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n",
    "\n",
    "print('最频繁的20个单词：')\n",
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试和验证文本中可能出现训练集中没有的单词，另外在训练时为了满足批量输入需要将所有或一个批次的文本长度对齐，因此上述字典的建立中会自动加入特殊标记_&lt;unk&gt;_ 和*&lt;pad&gt;* ，用来表示未知字符和填充字符。\n",
    "\n",
    "字典长度比较大，可能需要去除一些，或者使用预训练词向量初始化。\n",
    "\n",
    "**下面我们需要建立迭代器**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 36])\n",
      "tensor([[  263,     6,  4886,  ...,     4, 24290,    39],\n",
      "        [  492,     3,    16,  ..., 11775,     4,    14],\n",
      "        [   11,   159,   123,  ...,  2405,   403,     4],\n",
      "        ...,\n",
      "        [ 6532,     5,   760,  ...,     1,     1,     1],\n",
      "        [ 6451,     2,   298,  ...,     1,     1,     1],\n",
      "        [77674,     0,     0,  ...,     1,     1,     1]])\n"
     ]
    }
   ],
   "source": [
    "train_iterator, valid_iterator, test_iterator =data.BucketIterator.splits(\n",
    "    (train_data,valid_data,test_data),\n",
    "    batch_size=bs,device=device,shuffle=True)\n",
    "\n",
    "#测试\n",
    "for x in test_iterator:\n",
    "    print(x.text.shape)\n",
    "    print(x.text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值得注意的是，**迭代器中的文本已经被转换成了序号**，torchtext内部具体怎么实现的不清楚。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "定义一个LSTM模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple_rnn(\n",
      "  (embed): Embedding(108284, 100)\n",
      "  (rnn): LSTM(100, 256, batch_first=True)\n",
      "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class simple_rnn(nn.Module):\n",
    "    \n",
    "    def __init__(self,d_vocab: int,d_embed:int ,d_hidden:int ,dropout:float,d_output:int,vectors=None):\n",
    "        super(simple_rnn, self).__init__()\n",
    "\n",
    "        self.d_hidden=d_hidden\n",
    "        self.embed=nn.Embedding(d_vocab,d_embed)\n",
    "        self.rnn=nn.LSTM(d_embed,d_hidden,batch_first=True)\n",
    "        self.fc=nn.Linear(d_hidden,d_output)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "        #self.init_weight(vectors)\n",
    "\n",
    "    def init_weight(self,vectors=None):\n",
    "        if vectors is not None:\n",
    "            self.embed.weight.data.copy_(vectors)\n",
    "            \n",
    "        initrange=0.1\n",
    "        self.fc.weight.data.uniform_(-initrange,initrange)\n",
    "        \n",
    "    def forward(self,x,hidden=None):\n",
    "        # input:(bs,1ength)\n",
    "        \n",
    "        embeded=self.dropout(self.embed(x)) #(bs,length,d_embed)\n",
    "        \n",
    "        if hidden is not None:\n",
    "            output,hidden=self.rnn(embeded,hidden)\n",
    "        else:\n",
    "            output,(hidden,_)=self.rnn(embeded)\n",
    "        #output:(bs,length,d_hidden)\n",
    "        #hidden:(batch,1,d_hidden)\n",
    "\n",
    "        assert torch.equal(output[:,-1,:],hidden.squeeze(0))\n",
    "\n",
    "        return self.fc(hidden.squeeze(0))#(batch,d_output)\n",
    "    \n",
    "model=simple_rnn(d_vocab,d_embed,d_hidden,dropout,d_output)\n",
    "print(model)\n",
    "if use_cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试一下能否跑通"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 2])\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if use_cuda:\n",
    "    criterion.cuda()\n",
    "with torch.no_grad():\n",
    "    for batch in train_iterator:\n",
    "        x=batch.text\n",
    "        y=batch.label\n",
    "        if use_cuda:\n",
    "            x.cuda()\n",
    "            y.cuda()\n",
    "        preds=model(batch.text)\n",
    "        print(preds.shape)\n",
    "        criterion(preds,y.long())\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_iter, dev_iter, test_iter):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if use_cuda:\n",
    "        criterion.cuda()\n",
    "\n",
    "    # 学习率指数衰减，每次epoch：学习率 = gamma * 学习率\n",
    "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    dev_best_loss = float('inf')\n",
    "    last_improve = 0  # 记录上次验证集loss下降的batch数\n",
    "    #writer = SummaryWriter(log_dir=config.log_path + '/' + time.strftime('%m-%d.%H.%M', time.localtime())+'_'+which_data+'_'+which_model+'_'+which_task+'_'+exp_number)\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        train_loss=0\n",
    "        train_correct=0\n",
    "        # scheduler.step() # 学习率衰减\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            optimizer.zero_grad()\n",
    "            x=batch.text\n",
    "            y=batch.label\n",
    "            if use_cuda:\n",
    "                x.cuda()\n",
    "                y.cuda()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #训练集的准确率\n",
    "            true = y.data.cpu()\n",
    "            preds = torch.max(outputs.data, 1)[1].cpu()\n",
    "            train_correct+=(true==preds).sum()\n",
    "            train_loss+=loss.item()\n",
    "        train_loss/=len(train_iterator)   #train_loss\n",
    "        train_acc=train_correct/len(train_iterator.dataset)   #train_acc\n",
    "            \n",
    "        #验证集\n",
    "        dev_acc, dev_loss = evaluate(model, dev_iter)\n",
    "        if dev_loss < dev_best_loss:\n",
    "            dev_best_loss = dev_loss\n",
    "            improve = '*'\n",
    "            last_improve=epoch\n",
    "        else:\n",
    "            improve = ''\n",
    "        msg = 'Epoch: {0:>6},  Train Loss: {1:>5.2},  Train Acc: {2:>6.2%},  Val Loss: {3:>5.2},  Val Acc: {4:>6.2%} {5}'\n",
    "        print(msg.format(epoch+1, train_loss, train_acc, dev_loss, dev_acc, improve))\n",
    "        #writer.add_scalar(\"loss/train\", loss.item(), total_batch)\n",
    "        #writer.add_scalar(\"loss/dev\", dev_loss, total_batch)\n",
    "        #writer.add_scalar(\"acc/train\", train_acc, total_batch)\n",
    "        #writer.add_scalar(\"acc/dev\", dev_acc, total_batch)\n",
    "\n",
    "        if epoch - last_improve > require_improvement:\n",
    "            # 验证集loss超过1epoch没下降，结束训练\n",
    "            print(\"No optimization for a long time, auto-stopping...\")\n",
    "            break\n",
    "    #writer.close()\n",
    "    test(model, test_iter)\n",
    "\n",
    "def evaluate(model, data_iter, test=False):\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    with torch.no_grad():\n",
    "        for batch in data_iter:\n",
    "            x=batch.text\n",
    "            labels=batch.label\n",
    "            if use_cuda:\n",
    "                x.cuda()\n",
    "                labels.cuda()\n",
    "            outputs = model(x)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss_total += loss\n",
    "            labels = labels.data.cpu().numpy()\n",
    "            predic = torch.max(outputs.data, 1)[1].cpu().numpy()\n",
    "            labels_all = np.append(labels_all, labels)\n",
    "            predict_all = np.append(predict_all, predic)\n",
    "    model.train()\n",
    "    acc = metrics.accuracy_score(labels_all, predict_all)\n",
    "    \n",
    "    if test:\n",
    "        report = metrics.classification_report(labels_all, predict_all, labels=[i for i in range(len(class_list))],target_names=class_list, digits=4,output_dict=True)\n",
    "        confusion = metrics.confusion_matrix(labels_all, predict_all)\n",
    "        return acc, loss_total / len(data_iter), report, confusion\n",
    "    \n",
    "    return acc, loss_total / len(data_iter)\n",
    "\n",
    "\n",
    "def test(model, test_iter):\n",
    "    test_acc, test_loss, test_report, test_confusion = evaluate(model, test_iter, test=True)\n",
    "    msg = 'Test Loss: {0:>5.2},  Test Acc: {1:>6.2%}'\n",
    "    print(msg.format(test_loss, test_acc))\n",
    "    print(\"Precision, Recall and F1-Score...\")\n",
    "    print(test_report)\n",
    "    print(\"Confusion Matrix...\")\n",
    "    print(test_confusion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
